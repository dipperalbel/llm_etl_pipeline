[![Coverage](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/dipperalbel/adfcd8d043e30baf54876431640c2936/raw/coverage.json)](https://github.com/dipperalbel/llm_etl_pipeline/actions)

# Documentation

You can check the documentation here: [dipperalbel.github.io/llm_etl_pipeline/](https://dipperalbel.github.io/llm_etl_pipeline/)

## Assumptions

This project operates under the following key assumptions regarding the input PDF files and project structure:

* **Textual PDFs:** All input PDF documents are assumed to be *text-searchable* (i.e., not scanned images). The project relies on the ability to extract raw text content directly from the PDFs. If scanned PDFs are provided, text extraction will fail.
* **English Language Content:** The textual content within all input PDFs is assumed to be primarily in the English language. Text processing and analysis steps (e.g., keyword extraction, natural language processing) may yield inaccurate or irrelevant results for content in other languages.
* **Consistent Document Structure:** Particularly for **"call for proposals"** PDFs, a very similar internal structure and layout are assumed. The project's parsing logic relies on this consistency to accurately locate and extract specific pieces of information. Deviations in structure of the call for proposal PDFs may lead to incomplete or incorrect data extraction.
* **Presence of Call Proposals:** For each EU project intended for processing, it's assumed that a corresponding 'call for proposal' PDF file exists within the designated input folder. This PDF must contain the string "call" in its filename.
* **Handling of Numbered Call Files:** In cases where multiple PDF files exist for the same call, identified by a common naming pattern like `PROGRAMCODE-YYYY-TYPE-GRANT-CATEGORY-XX` (e.g., `AMIF-2025-TF2-AG-INTE-01`, `AMIF-2025-TF2-AG-INTE-02`), the project will only process the file with the *lowest numerical suffix* (XX). This is due to the assumption that such sequentially numbered files contains identical core information. For example, the call for proposal PDFs for `AMIF-2025-TF2-AG-INTE-01-WOMEN`, `AMIF-2025-TF2-AG-INTE-02-HEALTH`,... contain identical core information despite their differing specific extensions.
* **Currency Denomination:** All monetary values (e.g., prices, budgets, grants) mentioned within the PDF documents are assumed to be denominated in **Euros (EUR)**.

## Solution Overview

During the analysis of the AMIF grant project PDFs, it became clear that they do not contain project-specific **Technology Readiness Level (TRL)** information. While TRL is a standard concept in Horizon Europe projects, the documents only provided generic definitions (TRL 1 to 9) without practical application to the projects themselves. Our attempts to extract any TRL data, even using advanced LLM AI models like Gemini, were unsuccessful and often resulted in hallucinatory outputs. This outcome is consistent with our understanding from both internet research and Gemini, which suggests that TRL — a metric primarily focused on technological maturity — is not typically indicated in AMIF grant projects, given their social rather than technological nature.

Consequently, the solution prioritizes the extraction of the following data points:

* **Budget Information:** This includes detailed proposal budget and grant amounts per project, which are consistently and clearly documented within the **"call for proposal" PDFs**.
* **Organization Details:** We also aimed to extract the number and type of organizations involved in the grants. However, this proved challenging due to the ambiguity and lack of clear definitions in the task documentation. Without a precise understanding of what "number and type of organization" specifically entails within the grant context, we couldn't fully implement this extraction or clarify it through further inquiry.

Given that many of the provided PDFs were found to be templates or contained minimal additional data relevant to the extraction goals, the core focus of this solution was directed exclusively towards processing the "call for proposal" PDFs, as they proved to be the most valuable source of actionable information.

## Project Structure and Overview

This project is organized into several key directories and files to facilitate its ETL (Extract, Transform, Load) operations, documentation, and development practices.

The top-level structure includes:

* **`.github/workflows/`**: Contains the Continuous Integration (CI) workflow definitions.
* **`docs/`**: Houses the necessary code and configurations for creating and developing project documentation using Sphinx.
* **`logs/`**: This directory serves as the storage location for log files generated by the custom `loguru` logger.
* **`llm_etl_pipeline/`**: The core directory containing the main ETL application logic.
* **`.pre-commit-config.yaml`**: Configures pre-commit hooks that automate steps like code linting and import sorting, enforcing code style and quality before commits.

The **`__main__.py`** file at the root of the project orchestrates the entire ETL process. Users can execute this file by providing the input directory path via the command line. This file, while currently serving as the main execution point, can be considered an early version of an orchestration layer. Due to time constraints, a more sophisticated orchestration layer was not implemented, but the modular design of the `llm_etl_pipeline` directory aims to provide independent, high-level abstract building blocks for developing custom ETL pipelines.

The `llm_etl_pipeline` directory itself is organized into the following key sub-folders, each developed to be independent and serve a distinct purpose:

* **`customized_loggers/`**: This Python package is responsible for setting up and managing a customized `loguru` logger.  It outputs logs to the console and simultaneously stores them in the `/logs` directory at the project's root.
* **`extraction/`**: Contains the core methods and classes dedicated to the extraction of raw data from the PDF documents, including PDF conversion and text segmentation (Extraction).
* **`transformation/`**: This directory contains the classes and functions responsible for transforming and validating the extracted data. This includes data cleaning and deduplication logic (Transformation).
* **`typings/`**: This folder is dedicated to custom Python type hints, which facilitate dynamic analysis checks across the project, particularly within the `extraction` and `transformation` packages.

Within the `extraction`, `transformation`, and `typings` sub-folders, you will find additional sub-folders named `public` and `internal`. These indicate whether the methods and classes within are designed for public consumption or internal project use, respectively.

**Notably, a dedicated `load` subfolder has not yet been developed due to time constraints.** For now, the processed Pandas DataFrames are simply stored using their `to_csv` method, simplifying the initial data persistence approach.

### Data Validation with Pydantic

Data validation is a cornerstone of this project's reliability. To enforce strict data integrity, we've heavily leveraged the `Pydantic` package.

Every public method in the project is adorned with a call_validate decorator, ensuring that data conforms to predefined schemas at the point of invocation. We've applied this rigorous validation and immutability to public interfaces, while private methods do not have these same constraints.

Furthermore, most of the project's classes, such as Document and MonetaryInformation, were designed with immutability in mind. This means that once an object of these classes is initialized, its field values cannot be altered, preventing unintended data corruption.


### Key Classes

The core functionality of this project is encapsulated within several key classes, each designed for a specific stage of the data processing pipeline:

* **`PdfConverter`**: This class is responsible for the initial step of converting PDF files into a raw string format, making their content accessible for further processing.

* **`Document`**: The `Document` class is central to text handling. Its primary purpose is to segment the raw text into logical paragraphs. Furthermore, it provides functionality to query these paragraphs using regular expressions, allowing for the extraction of sub-filtered text sections. A crucial design aspect of this class is its **immutability**: once a `Document` object is initialized, its field values cannot be altered, ensuring data integrity.

* **`LocalLLM`**: This class facilitates interaction with a local Large Language Model (LLM). Its sole purpose is to perform queries against the local LLM, enabling the application of advanced natural language processing capabilities.

* **`Pipeline`**: Designed for the data transformation phase, the `Pipeline` class allows for the consecutive application of a series of functions to an input Pandas DataFrame. Once a `Pipeline` object is initialized with the desired sequence of transformation functions, it can be efficiently reused to process data through the defined steps. **Crucially, the `Pipeline` class currently enforces that all accepted functions must have a signature indicating a Pandas DataFrame as input and returning a Pandas DataFrame, a requirement that is also verified at runtime.**

### Testing Strategy

For quality assurance, a suite of unit tests has been developed to validate individual components and functions of the codebase. While these tests provide foundational coverage, the current test coverage stands at approximately 50%, indicating areas for future expansion.

## Design Choices and Approach

### Data Extraction Flow and Temporary Storage

The core of this solution for information extraction relies on a multi-stage process leveraging local Large Language Models (LLMs) for specific data points. Our approach prioritizes accuracy and efficiency through a combination of heuristic text processing and targeted LLM inference.

* **PDF to Text Conversion:**
    The process begins with converting the PDF documents into raw text strings. This is handled by the `PdfConverter` class, which internally uses the `docling` package for robust text extraction from PDF files.

* **Text Segmentation - Paragraphs:**
    Following text conversion, the raw text undergoes segmentation into paragraphs using the `Document` class. Although various methods for defining paragraphs were explored—including single newlines (`\n`), empty lines (`\n\n`), and advanced models like `SaT (wtpsplit)` (https://github.com/segment-any-text/wtpsplit) — an heuristic approach based on empty lines (`\n\n`) was ultimately adopted. This method demonstrated superior performance in accurately identifying distinct paragraphs, a performance that is, of course, dependent on the initial text extraction quality from the PDFs.

* **Text Segmentation - Sentences:**
    After paragraph definition, sentences are extracted from each paragraph. For this granular segmentation, the set of `SaT (wtpsplit)` models (https://github.com/segment-any-text/wtpsplit) were employed due to their effectiveness in delineating individual sentences.

* **Information Filtering with Regular Expressions:**
    Before LLM processing, the segmented text (primarily paragraphs, though sentence-level filtering is also an option) undergoes a crucial filtering step using Regular Expressions. These regex patterns were custom-designed based on common characteristics observed in "call for proposal" PDFs to pre-select relevant sections. This includes identifying:
    * **Monetary Amounts:** Strings containing currency indicators (e.g., "EUR") coupled with digits.
    * **Consortium Details:** Sections typically related to consortium formation, specifically looking for the table that indicate minimum number of entities.

* **LLM-based Data Extraction:**
    Once filtered, the relevant paragraphs are fed to the pre-selected local LLMs.
    * **Granular Processing:** To maximize extraction accuracy, particularly for monetary information, paragraphs are inputted to the LLMs in batches rather than providing the entire document at once. This granular approach was observed to yield more precise results (at least for local LLM). For consortium entity extraction, the LLM receives the identified table as its input.
    * **Prompt Engineering:** User and system prompts for the LLMs are dynamically generated using a `Jinja2` template.
    * **Structured Output:** The LLM's raw output is then parsed using a `PydanticJsonParser`. This ensures that the extracted data conforms to a predefined schema, enabling robust validation and easy integration into subsequent processes. However, it is important to note that a well-defined fallback method is currently not in place to handle `ValidationError` instances raised by the parser.
    * **Iterative Accumulation:** This batch processing, prompting, and parsing cycle is repeated for all filtered paragraphs, and the results are accumulated to form the complete extracted dataset for the document.

The entire extraction process described above is repeated for **each individual call for proposal PDF document**. The extracted data from each PDF is then temporarily stored in two separate JSON files: one for monetary information and another for entity-related data.

#### NOTES

* **Local LLM Models:** We utilized `phi4:14b` primarily for extracting monetary information and `gemma3:27b` for processing consortium-related table data.

### Data Transformation

Following the extraction phase, the stored JSON files are loaded into `pandas` DataFrames for subsequent transformation and consolidation. This stage is crucial for refining the extracted raw data:

* **Monetary Data Transformation:**
    Extracted monetary data undergoes various validation checks to ensure its quality and adherence to expected conditions. We then filter and retain only the information most relevant for analysis, such as the grant requested per project, available call budgets, or specific budget allocations per topic as mentioned in the call for proposal.
    A significant challenge identified was the **duplication of monetary values**, where identical amounts might or might not refer to the same underlying entity or concept (e.g., two mentions of the minimum EU grant request). To address this, a specific deduplication strategy is employed:
    1.  Sentences containing the duplicate monetary amounts are converted into embeddings using `Sentence-Transformers (S-BERT)`(https://github.com/UKPLab/sentence-transformers).
    2.  Hierarchical clustering is then performed on these embeddings using cosine distance.
    3.  If multiple sentences fall within the same cluster (indicating high semantic similarity for the same amount), the sentence with the *longest text* is selected as the representative for that cluster, simplifying the data while retaining context.

* **Entity Data Transformation:**
    For the extracted entity data, due to time constraints, the transformation primarily involves basic validation checks followed by a simple stacking of the type of organization data extracted.

* **Pipeline Orchestration:** All these transformation steps are orchestrated via a `Pipeline` class, which applies a series of pre-written functions sequentially to the `pandas` DataFrames, streamlining the data processing workflow.
 
### Data Load

Finally, the processed `pandas` DataFrames for monetary and organization type information are stored as CSV files: `etl_money_result.csv` for the monetary data, and `etl_entity_result.csv` for the entity data.

## Installation

Instructions on how to get your project up and running.

### Prerequisites

- Python >=3.10,<3.14
- poetry

### Install using PyPI

1.  **Install the package:**
    You can install the `llm_etl_pipeline` package directly using pip:
    ```bash
    pip install llm_etl_pipeline
    ```
### Install locally (recommended)

1. **Clone the repository:**
    Begin by cloning the project's Git repository to your local machine:
    ```bash
    git clone https://github.com/dipperalbel/llm_etl_pipeline
    cd your-project-name
    ```

2.  **Install dependencies with Poetry:**
    Navigate into the cloned project directory. Poetry will automatically create a virtual environment and install all project dependencies defined in `pyproject.toml` (and locked in `poetry.lock`):
    ```bash
    poetry install
    ```
    
3.  **Activate the virtual environment (optional, but good practice):**
    While you can run commands directly via `poetry run`, you can also activate the virtual environment managed by Poetry:
    ```bash
    poetry shell
    ```
    Once activated, your terminal prompt might change to indicate the active environment. To exit, simply type `exit`.

### Ollama Model Installation

**IMPORTANT:** This project requires specific LLM models to be available through your Ollama installation. After installing Ollama, you need to download `phi4:14b` and `gemma3:27b` using the Ollama command-line interface:

```bash
ollama pull phi4:14b
ollama pull gemma3:27b
```

## Usage

To use this project, you will primarily interact with its `__main__` file from your terminal.

### Running the Extraction Process

1.  **Ensure your Ollama server is running** and the required models (`phi4:14b` and `gemma3:27b`) are pulled.
2.  **Activate your Poetry shell** (if you haven't already):
    ```bash
    poetry shell
    ```
3.  **Execute the main script:**
    ```bash
    poetry run python main.py 
    ```
    
4.  **Provide the PDF directory path:**
    The script will prompt you to enter the path to the directory containing the PDF documents. MAKE SURE THAT THE INPUT FOLDER CONTAINS THE CALL FOR PROPOSAL PDFS (the PDFs should be like AMIF-2024-TF2-AG-INFO-01_separator_call-...).
    ```
    Please enter the path to the directory containing the PDF documents: 
    ```
    You should enter the full path to your PDF folder, for example:
    * **On Windows:** `C:\path\to\your\pdf_documents`
    * **On Linux/macOS:** `/home/user/path/to/your/pdf_documents`

### Output Files

Upon successful completion of the extraction and processing, two CSV files will be generated in the root directory:

* `etl_money_result.csv`: This file contains the extracted and processed monetary information. It includes the following columns:
    * `document_id`: The EU grant project identifier (e.g., `AMIF-2024-TF2-AG-THB`).
    * `value`: The extracted monetary amount.
    * `currency`: The currency associated with the `value`.
    * `context`: The motivation or context for the extracted amount.
    * `original_sentence`: The original sentence from the input text where the amount was found.

* `etl_entity_result.csv`: This file contains the extracted and validated entity data. It includes the following columns:
    * `document_id`: The EU grant project identifier.
    * `organization_type`: A list of organization types found in the consortium table of the proposal PDF.
    * `min_entities`: The minimum number of entities, as indicated in the entities row of the consortium table.

## Acknowledgments

We gratefully acknowledge the work of **Shcherbak AI AS** for developing **ContextGem** (https://github.com/shcherbak-ai/contextgem). Some parts of the code for this project are based on ContexGem.
